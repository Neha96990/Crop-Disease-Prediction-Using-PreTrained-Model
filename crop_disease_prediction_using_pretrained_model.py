# -*- coding: utf-8 -*-
"""Crop_Disease_Prediction_Using_PreTrained_Model.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1317lNS4nO2vwOC2QDO2spToAzbBzLEdY
"""

from tensorflow.keras.applications import MobileNetV2
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.preprocessing.image import ImageDataGenerator

# Training data generator with augmentation
train_datagen = ImageDataGenerator(
    rescale=1./255,
    shear_range=0.2,
    zoom_range=0.2,
    horizontal_flip=True
)

# Validation data generator without augmentation
val_datagen = ImageDataGenerator(rescale=1./255)

# Create generators
train_generator = train_datagen.flow_from_directory(
    '/content/drive/MyDrive/Train/Train',
    target_size=(225, 225),
    batch_size=32,
    class_mode='categorical'
)

validation_generator = val_datagen.flow_from_directory(
    '/content/drive/MyDrive/Validation/Validation',
    target_size=(225, 225),
    batch_size=32,
    class_mode='categorical'
)

# Load MobileNetV2 base model
base_model = MobileNetV2(weights='imagenet', include_top=False, input_shape=(225, 225, 3))

# Freeze the base model
base_model.trainable = False

from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model

# Add custom head
x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.3)(x)
predictions = Dense(3, activation='softmax')(x)  # Assuming 3 classes

# Final model
model = Model(inputs=base_model.input, outputs=predictions)

from tensorflow.keras.optimizers import Adam

model.compile(
    optimizer=Adam(learning_rate=0.0001),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    train_generator,
    epochs=5,
    validation_data=validation_generator
)

# Unfreeze the base model for fine-tuning
base_model.trainable = True

# Recompile the model with a lower learning rate
model.compile(
    optimizer=Adam(learning_rate=1e-5),
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

# Continue training (fine-tuning)
history_fine = model.fit(
    train_generator,
    epochs=5,
    validation_data=validation_generator
)

model.save("mobilenetv2_plant_disease.h5")

from tensorflow.keras.preprocessing.image import load_img, img_to_array
import numpy as np

def preprocess_image(image_path, target_size=(225, 225)):
    img = load_img(image_path, target_size=target_size)
    x = img_to_array(img)
    x = x.astype('float32') / 255.
    x = np.expand_dims(x, axis=0)
    return x

# Load and predict
image = preprocess_image("/content/drive/MyDrive/Train/Train/Powdery/806d7d0df8cae3c2.jpg")
prediction = model.predict(image)

# Map prediction to label
labels = train_generator.class_indices
labels = {v: k for k, v in labels.items()}
predicted_label = labels[np.argmax(prediction)]

print("Predicted class:", predicted_label)

# Final accuracy from fine-tuning history
final_train_acc = history_fine.history['accuracy'][-1]
final_val_acc = history_fine.history['val_accuracy'][-1]

print(f"Final Training Accuracy: {final_train_acc * 100:.2f}%")
print(f"Final Validation Accuracy: {final_val_acc * 100:.2f}%")

import matplotlib.pyplot as plt

# Combine both training phases
all_accuracy = history.history['accuracy'] + history_fine.history['accuracy']
all_val_accuracy = history.history['val_accuracy'] + history_fine.history['val_accuracy']
all_loss = history.history['loss'] + history_fine.history['loss']
all_val_loss = history.history['val_loss'] + history_fine.history['val_loss']

epochs_range = range(1, len(all_accuracy) + 1)

plt.figure(figsize=(14, 6))

# Accuracy
plt.subplot(1, 2, 1)
plt.plot(epochs_range, all_accuracy, label='Training Accuracy')
plt.plot(epochs_range, all_val_accuracy, label='Validation Accuracy')
plt.title('Model Accuracy Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()

# Loss
plt.subplot(1, 2, 2)
plt.plot(epochs_range, all_loss, label='Training Loss')
plt.plot(epochs_range, all_val_loss, label='Validation Loss')
plt.title('Model Loss Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()

import matplotlib.pyplot as plt # Import matplotlib for image display

# Squeeze the image to remove the batch dimension
plt.imshow(np.squeeze(image))  # Use the original PIL image for display
plt.show()

